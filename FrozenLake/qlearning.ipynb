{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Frozen-Lake with Q-Learning (table based)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The environment\n",
    "\n",
    "(From https://www.gymlibrary.ml/environments/toy_text/frozen_lake/ ):\n",
    "\n",
    "Frozen lake involves crossing a frozen lake from Start(S) to Goal(G) without falling into any Holes(H) by walking over the Frozen(F) lake. The agent may not always move in the intended direction due to the slippery nature of the frozen lake.\n",
    "\n",
    "### Action Space\n",
    "\n",
    "The agent takes a 1-element vector for actions. The action space is ```(dir)```, where ```dir``` decides direction to move in which can be:\n",
    "\n",
    "    0: LEFT\n",
    "\n",
    "    1: DOWN\n",
    "\n",
    "    2: RIGHT\n",
    "\n",
    "    3: UP\n",
    "\n",
    "### Observation Space / State Space\n",
    "\n",
    "The observation/state is a value representing the agentâ€™s current position as current_row * nrows + current_col (where both the row and col start at 0). For example, the goal position in the 4x4 map can be calculated as follows: 3 * 4 + 3 = 15. The number of possible observations is dependent on the size of the map. For example, the 4x4 map has 16 possible observations.\n",
    "\n",
    "### Rewards\n",
    "\n",
    "Reward schedule:\n",
    "\n",
    "    Reach goal(G): +1\n",
    "\n",
    "    Reach hole(H): 0\n",
    "\n",
    "    Reach frozen(F): 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Interacting with OpenAI gym and the environment\n",
    "\n",
    "### Initialization\n",
    "\n",
    "We can create and render the environment like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "environment = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=True)\n",
    "\n",
    "def init_environment(env: gym.Env):\n",
    "    state, _ = env.reset(return_info=True)  # Restart/initialize the environment\n",
    "    print(env.render(mode=\"ansi\"))\n",
    "\n",
    "    # The state returned from environment.reset() is our initial state:\n",
    "    print(state)\n",
    "\n",
    "init_environment(environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Moving / Taking an action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "# We can get the action-space with environment.action_space\n",
    "print(environment.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New state : 0\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To move around (taking a series of actions), we use the ```environment.step()``` function:\n",
    "def move(env: gym.Env):\n",
    "    new_state, reward, done, _ = env.step(0)  # 0 is left, remember that we can use all 4 actions from the action-space, and that there is a chance of slipping\n",
    "\n",
    "    # We got three new variables:\n",
    "    print(f\"New state : {new_state}\")  # Updated state after moving\n",
    "    print(f\"Reward: {reward}\")  # The reward we got from the environment\n",
    "    print(f\"Done: {done}\")  # If the game is finished or not\n",
    "    print(\"\")\n",
    "    print(env.render(mode=\"ansi\"))  # Re-render environment after moving\n",
    "\n",
    "move(environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The Q-Table\n",
    "\n",
    "We use a q-table to help guide us to the best action to take at each timestep.\n",
    "It is just a simple lookup table containing the maximum expected future rewards for all actions in all states.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create an empty q-table of size state-space x action-space\n",
    "def initialize_q_table(env: gym.Env) -> np.array:\n",
    "    return np.array([[0 for _ in range(env.action_space.n)] for _ in range(env.observation_space.n)], dtype=np.float32)\n",
    "\n",
    "q_table = initialize_q_table(environment)\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Policies\n",
    "\n",
    "A policy is a method that maps from a state to an action\n",
    "This is what decides how we play the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We first define the optimal policy, this policy always picks what it believes is the \"best\" \n",
    "# action in the current state\n",
    "def optimal_policy(env: gym.Env, q_sa: np.array, s: int) -> int:\n",
    "    \"\"\"RL-policy for optimal play.\n",
    "\n",
    "    Args:\n",
    "        env: Frozen-lake Environment\n",
    "        q_sa: q-table\n",
    "        s: state\n",
    "\n",
    "    Returns:\n",
    "        optimal action for given state and q-table.\n",
    "    \"\"\"\n",
    "    if np.all(q_sa[s] == q_sa[s][0]):  # If all q-values are equal (e.g. all 0), we cannot differentiate\n",
    "        return env.action_space.sample()  # Pick a random action\n",
    "    return int(np.argmax(q_sa[s]))  # Return the argument (element number) with the highest q-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# When we are training, we also want some exploration (see exploration/exploitation tradeoff)\n",
    "def epsilon_greedy_policy(env: gym.Env, q_sa: np.array, s: int, eps: float = 0.15) -> int:\n",
    "    \"\"\"RL-policy for exploration/exploitation play.\n",
    "\n",
    "    Args:\n",
    "        env: Frozen-lake Environment\n",
    "        q_sa: q-table\n",
    "        s: state\n",
    "        eps: exploration chance\n",
    "\n",
    "    Returns:\n",
    "        either random action, or optimal action for given state and q-table.\n",
    "    \"\"\"\n",
    "    if np.random.rand() < eps:  # If a random number n is lower than eps:\n",
    "        return env.action_space.sample()  # Pick a random action\n",
    "    return optimal_policy(env, q_sa, s)  # Otherwise, play optimally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We can also have a decaying explore strategy, to promote exploration early on and exploitation later\n",
    "def decaying_epsilon_greedy_policy(env: gym.Env, q_sa: np.array, s: int, \n",
    "                                   episode: int, max_episodes: int, \n",
    "                                   max_eps: float = 0.8, min_eps: float = 0.02) -> int:\n",
    "    \"\"\"RL-policy for exploration/exploitation play.\n",
    "\n",
    "    Args:\n",
    "        env: Frozen-lake Environment\n",
    "        q_sa: q-table\n",
    "        s: state\n",
    "        episode: current timestep\n",
    "        max_episodes: maximum timestep\n",
    "        max_eps: max exploration chance\n",
    "        min_eps: min exploration chance\n",
    "\n",
    "    Returns:\n",
    "        either random action, or optimal action for given state and q-table.\n",
    "    \"\"\"\n",
    "    eps = min_eps + (max_eps - min_eps) * ((max_episodes - episode) / max_episodes)\n",
    "    if np.random.rand() < eps:  # If a random number n is lower than eps:\n",
    "        return env.action_space.sample()  # Pick a random action\n",
    "    return optimal_policy(env, q_sa, s)  # Otherwise, play optimally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Playing FrozenLake with a policy\n",
    "\n",
    "To play FrozenLake with a q-table and a policy, we just replace the action in the earlier step with the action from the policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def play():\n",
    "    max_steps = 20  # Maximum steps to play\n",
    "    state, _ = environment.reset(return_info=True)  # Restart/initialize the environment\n",
    "    print(environment.render(mode=\"ansi\"))\n",
    "    for _ in range(max_steps):\n",
    "        action = optimal_policy(environment, q_table, state)  # Chose the optimal action based on values from the q-table\n",
    "        new_state, reward, done, _ = environment.step(action)  # Play using that action\n",
    "        print(environment.render(mode=\"ansi\"))\n",
    "\n",
    "        # We stop the game if we are finished\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        state = new_state  # If not, replace the state with the new state before next step\n",
    "\n",
    "play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Learning from experience / Updating the q-table\n",
    "\n",
    "Right now the q-table is filled with zeroes, and does not update, we want to update this for every step we take based on the td-error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The td-error is based on a single experience, i.e. a e_t = (s_t, a_t, r_t, s_(t+1)) tuple, \n",
    "# for the experience at timestep t\n",
    "# We can save this in a dataclass:\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Experience:\n",
    "    __slots__ = (\"state\", \"action\", \"reward\", \"new_state\", \"done\")  \n",
    "    # Optimization so that we can save thousands of these objects later\n",
    "    state: int\n",
    "    action: int\n",
    "    reward: float\n",
    "    new_state: int\n",
    "    done: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We can then calculate the td-error and q-update of a single experience\n",
    "def q_temporal_difference(q_sa: np.array, experience: Experience, alpha: float = 0.85, gamma: float = 0.98) -> float:\n",
    "    \"\"\"Calculates the q-update.\n",
    "\n",
    "\tArgs:\n",
    "\t\tq_sa: q-table\n",
    "\t\texperience: a single experience\n",
    "\t\talpha: learning-rate\n",
    "\t\tgamma: discount\n",
    "\n",
    "\tReturns:\n",
    "\t\tq-td update value\n",
    "    \"\"\"\n",
    "    td_error = experience.reward + gamma * np.max(q_sa[experience.new_state]) - q_sa[experience.state][experience.action]\n",
    "    return q_sa[experience.state][experience.action] + alpha * td_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Using the q-update, we can update our q-table over multiple games\n",
    "def q_learning(env: gym.Env, q_sa: np.array, n_episodes: int = 100000, m_ep_length: int = 200) -> np.array:\n",
    "    \"\"\"q-learning implementation to update a q-table.\n",
    "\n",
    "\tArgs:\n",
    "\t\tenv: gym environment\n",
    "\t\tq_sa: initial q-table\n",
    "\t\tn_episodes: number of episodes to train on\n",
    "\t\tm_ep_length: maximum episode length\n",
    "\n",
    "\tReturns:\n",
    "\t\tupdated q-table\n",
    "    \"\"\"\n",
    "    for episode in range(n_episodes):\n",
    "        s, _ = env.reset(return_info=True)  # Restart/initialize the environment\n",
    "        for _ in range(m_ep_length):\n",
    "            a = decaying_epsilon_greedy_policy(env, q_sa, s, episode, n_episodes)  # Exploration strategy\n",
    "            s_new, r, d, _ = env.step(a)  # Play using that action\n",
    "\n",
    "            exp = Experience(s, a, r, s_new, d)  # We create an experience from this transition\n",
    "            q_td = q_temporal_difference(q_sa, exp)  # We calculate the q-update\n",
    "            q_sa[s][a] = q_td  # We update the q-table\n",
    "\n",
    "            # We stop the game if we are finished\n",
    "            if d:\n",
    "                break\n",
    "\n",
    "            s = s_new  # If not, replace the state with the new state before next step\n",
    "    return q_sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Now we can try:\n",
    "q_table = initialize_q_table(environment)\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.54 0.39 0.24 0.29]\n",
      " [0.   0.06 0.06 0.36]\n",
      " [0.18 0.15 0.04 0.15]\n",
      " [0.   0.01 0.01 0.15]\n",
      " [0.64 0.01 0.01 0.38]\n",
      " [0.   0.   0.   0.  ]\n",
      " [0.13 0.   0.   0.  ]\n",
      " [0.   0.   0.   0.  ]\n",
      " [0.12 0.1  0.02 0.59]\n",
      " [0.11 0.73 0.11 0.09]\n",
      " [0.94 0.   0.   0.06]\n",
      " [0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.  ]\n",
      " [0.63 0.7  0.83 0.11]\n",
      " [0.29 1.   0.2  0.35]\n",
      " [0.   0.   0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "q_table = q_learning(environment, q_table, n_episodes=300000)  # 100k episodes will take some time, lower to see how it works (though it might not converge)\n",
    "print(np.around(q_table, 2))  # We round to two decimal places for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can now redo the \"Playing FrozenLake with a policy\" step, and you have a fully working q-learning agent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
