{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Frozen-Lake with Q-Learning (table based)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The environment\n",
    "\n",
    "(From https://www.gymlibrary.ml/environments/toy_text/frozen_lake/ ):\n",
    "\n",
    "Frozen lake involves crossing a frozen lake from Start(S) to Goal(G) without falling into any Holes(H) by walking over the Frozen(F) lake. The agent may not always move in the intended direction due to the slippery nature of the frozen lake.\n",
    "\n",
    "### Action Space\n",
    "\n",
    "The agent takes a 1-element vector for actions. The action space is ```(dir)```, where ```dir``` decides direction to move in which can be:\n",
    "\n",
    "    0: LEFT\n",
    "\n",
    "    1: DOWN\n",
    "\n",
    "    2: RIGHT\n",
    "\n",
    "    3: UP\n",
    "\n",
    "### Observation Space / State Space\n",
    "\n",
    "The observation/state is a value representing the agentâ€™s current position as current_row * nrows + current_col (where both the row and col start at 0). For example, the goal position in the 4x4 map can be calculated as follows: 3 * 4 + 3 = 15. The number of possible observations is dependent on the size of the map. For example, the 4x4 map has 16 possible observations.\n",
    "\n",
    "### Rewards\n",
    "\n",
    "Reward schedule:\n",
    "\n",
    "    Reach goal(G): +1\n",
    "\n",
    "    Reach hole(H): 0\n",
    "\n",
    "    Reach frozen(F): 0\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Interacting with OpenAI gym and the environment\n",
    "\n",
    "### Initialization\n",
    "\n",
    "We can create and render the environment like this:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "environment = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=True)\n",
    "\n",
    "def init_environment(env: gym.Env):\n",
    "    state, _ = env.reset(return_info=True)  # Restart/initialize the environment\n",
    "    print(env.render(mode=\"ansi\"))\n",
    "\n",
    "    # The state returned from environment.reset() is our initial state:\n",
    "    print(state)\n",
    "\n",
    "init_environment(environment)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Moving / Taking an action\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "# We can get the action-space with environment.action_space\n",
    "print(environment.action_space)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New state : 0\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "\n",
      "  (Left)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To move around (taking a series of actions), we use the ```environment.step()``` function:\n",
    "def move(env: gym.Env):\n",
    "    new_state, reward, done, _ = env.step(0)  # 0 is left, remember that we can use all 4 actions from the action-space, and that there is a chance of slipping\n",
    "\n",
    "    # We got three new variables:\n",
    "    print(f\"New state : {new_state}\")  # Updated state after moving\n",
    "    print(f\"Reward: {reward}\")  # The reward we got from the environment\n",
    "    print(f\"Done: {done}\")  # If the game is finished or not\n",
    "    print(\"\")\n",
    "    print(env.render(mode=\"ansi\"))  # Re-render environment after moving\n",
    "\n",
    "move(environment)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Q-Table\n",
    "\n",
    "We use a q-table to help guide us to the best action to take at each timestep.\n",
    "It is just a simple lookup table containing the maximum expected future rewards for all actions in all states.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create an empty q-table of size state-space x action-space\n",
    "def initialize_q_table(env: gym.Env) -> np.array:\n",
    "    return np.array([[0 for _ in range(env.action_space.n)] for _ in range(env.observation_space.n)], dtype=np.float32)\n",
    "\n",
    "q_table = initialize_q_table(environment)\n",
    "print(q_table)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Policies\n",
    "\n",
    "A policy is a method that maps from a state to an action\n",
    "This is what decides how we play the game"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "# We first define the optimal policy, this policy always picks what it believes is the \"best\" action in the current state\n",
    "def optimal_policy(env: gym.Env, q_sa: np.array, s: int) -> int:\n",
    "    \"\"\"RL-policy for optimal play.\n",
    "\n",
    "    Args:\n",
    "        env: Frozen-lake Environment\n",
    "        q_sa: q-table\n",
    "        s: state\n",
    "\n",
    "    Returns:\n",
    "        optimal action for given state and q-table.\n",
    "    \"\"\"\n",
    "    if np.all(q_sa[s] == q_sa[s][0]):  # If all q-values are equal (e.g. all 0), we cannot differentiate\n",
    "        return env.action_space.sample()  # Pick a random action\n",
    "    return int(np.argmax(q_sa[s]))  # Return the argument (element number) with the highest q-value"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "# When we are training, we also want some exploration (see exploration/exploitation tradeoff)\n",
    "def epsilon_greedy_policy(env: gym.Env, q_sa: np.array, s: int, eps: float = 0.15) -> int:\n",
    "    \"\"\"RL-policy for exploration/exploitation play.\n",
    "\n",
    "    Args:\n",
    "        env: Frozen-lake Environment\n",
    "        q_sa: q-table\n",
    "        s: state\n",
    "        eps: exploration chance\n",
    "\n",
    "    Returns:\n",
    "        either random action, or optimal action for given state and q-table.\n",
    "    \"\"\"\n",
    "    if np.random.rand() < eps:  # If a random number n is lower than eps:\n",
    "        return env.action_space.sample()  # Pick a random action\n",
    "    return optimal_policy(env, q_sa, s)  # Otherwise, play optimally"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "# We can also have a decaying explore strategy, to promote exploration early on and exploitation later\n",
    "def decaying_epsilon_greedy_policy(env: gym.Env, q_sa: np.array, s: int, episode: int, max_episodes: int, max_eps: float = 0.8, min_eps: float = 0.02) -> int:\n",
    "    \"\"\"RL-policy for exploration/exploitation play.\n",
    "\n",
    "    Args:\n",
    "        env: Frozen-lake Environment\n",
    "        q_sa: q-table\n",
    "        s: state\n",
    "        episode: current timestep\n",
    "        max_episodes: maximum timestep\n",
    "        max_eps: max exploration chance\n",
    "        min_eps: min exploration chance\n",
    "\n",
    "    Returns:\n",
    "        either random action, or optimal action for given state and q-table.\n",
    "    \"\"\"\n",
    "    eps = min_eps + (max_eps - min_eps) * ((max_episodes - episode) / max_episodes)\n",
    "    if np.random.rand() < eps:  # If a random number n is lower than eps:\n",
    "        return env.action_space.sample()  # Pick a random action\n",
    "    return optimal_policy(env, q_sa, s)  # Otherwise, play optimally"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Playing FrozenLake with a policy\n",
    "\n",
    "To play FrozenLake with a q-table and a policy, we just replace the action in the earlier step with the action from the policy:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001B[41mH\u001B[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def play():\n",
    "    max_steps = 20  # Maximum steps to play\n",
    "    state, _ = environment.reset(return_info=True)  # Restart/initialize the environment\n",
    "    print(environment.render(mode=\"ansi\"))\n",
    "    for _ in range(max_steps):\n",
    "        action = optimal_policy(environment, q_table, state)  # Chose the optimal action based on values from the q-table\n",
    "        new_state, reward, done, _ = environment.step(action)  # Play using that action\n",
    "        print(environment.render(mode=\"ansi\"))\n",
    "\n",
    "        # We stop the game if we are finished\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        state = new_state  # If not, replace the state with the new state before next step\n",
    "\n",
    "play()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Learning from experience / Updating the q-table\n",
    "\n",
    "Right now the q-table is filled with zeroes, and does not update, we want to update this for every step we take based on the td-error."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "# The td-error is based on a single experience, i.e. a e_t = (s_t, a_t, r_t, s_(t+1)) tuple, for the experience at timestep t\n",
    "# We can save this in a dataclass:\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Experience:\n",
    "    __slots__ = (\"state\", \"action\", \"reward\", \"new_state\", \"done\")  # Optimization so that we can save thousands of these objects later\n",
    "    state: int\n",
    "    action: int\n",
    "    reward: float\n",
    "    new_state: int\n",
    "    done: bool"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "# We can then calculate the td-error and q-update of a single experience\n",
    "def q_temporal_difference(q_sa: np.array, experience: Experience, alpha: float = 0.85, gamma: float = 0.98) -> float:\n",
    "    \"\"\"Calculates the q-update.\n",
    "\n",
    "\tArgs:\n",
    "\t\tq_sa: q-table\n",
    "\t\texperience: a single experience\n",
    "\t\talpha: learning-rate\n",
    "\t\tgamma: discount\n",
    "\n",
    "\tReturns:\n",
    "\t\tq-td update value\n",
    "    \"\"\"\n",
    "    td_error = experience.reward + gamma * np.max(q_sa[experience.new_state]) - q_sa[experience.state][experience.action]\n",
    "    return q_sa[experience.state][experience.action] + alpha * td_error"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "# Using the q-update, we can update our q-table over multiple games\n",
    "def q_learning(env: gym.Env, q_sa: np.array, n_episodes: int = 100000, m_ep_length: int = 200) -> np.array:\n",
    "    \"\"\"q-learning implementation to update a q-table.\n",
    "\n",
    "\tArgs:\n",
    "\t\tenv: gym environment\n",
    "\t\tq_sa: initial q-table\n",
    "\t\tn_episodes: number of episodes to train on\n",
    "\t\tm_ep_length: maximum episode length\n",
    "\n",
    "\tReturns:\n",
    "\t\tupdated q-table\n",
    "    \"\"\"\n",
    "    for episode in range(n_episodes):\n",
    "        s, _ = env.reset(return_info=True)  # Restart/initialize the environment\n",
    "        for _ in range(m_ep_length):\n",
    "            a = decaying_epsilon_greedy_policy(env, q_sa, s, episode, n_episodes)  # Exploration strategy\n",
    "            s_new, r, d, _ = env.step(a)  # Play using that action\n",
    "\n",
    "            exp = Experience(s, a, r, s_new, d)  # We create an experience from this transition\n",
    "            q_td = q_temporal_difference(q_sa, exp)  # We calculate the q-update\n",
    "            q_sa[s][a] = q_td  # We update the q-table\n",
    "\n",
    "            # We stop the game if we are finished\n",
    "            if d:\n",
    "                break\n",
    "\n",
    "            s = s_new  # If not, replace the state with the new state before next step\n",
    "    return q_sa"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Now we can try:\n",
    "q_table = initialize_q_table(environment)\n",
    "print(q_table)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.54 0.39 0.24 0.29]\n",
      " [0.   0.06 0.06 0.36]\n",
      " [0.18 0.15 0.04 0.15]\n",
      " [0.   0.01 0.01 0.15]\n",
      " [0.64 0.01 0.01 0.38]\n",
      " [0.   0.   0.   0.  ]\n",
      " [0.13 0.   0.   0.  ]\n",
      " [0.   0.   0.   0.  ]\n",
      " [0.12 0.1  0.02 0.59]\n",
      " [0.11 0.73 0.11 0.09]\n",
      " [0.94 0.   0.   0.06]\n",
      " [0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.  ]\n",
      " [0.63 0.7  0.83 0.11]\n",
      " [0.29 1.   0.2  0.35]\n",
      " [0.   0.   0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "q_table = q_learning(environment, q_table, n_episodes=300000)  # 100k episodes will take some time, lower to see how it works (though it might not converge)\n",
    "print(np.around(q_table, 2))  # We round to two decimal places for readability"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can now redo the \"Playing FrozenLake with a policy\" step, and you have a fully working q-learning agent."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Frozen-Lake with a DQN (Deep Q-Network) agent\n",
    "\n",
    "## Neural Network predictions\n",
    "\n",
    "While we in the previous step used a q-table as backend, the problem can also be solved by function approximation with a neural network.\n",
    "\n",
    "### Building the model\n",
    "\n",
    "We can start by building the network model:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.optimizer_v2.adam import Adam\n",
    "\n",
    "def build_dqn_model(alpha: float = 0.001) -> Sequential:\n",
    "    \"\"\"Builds a deep neural net which predicts the Q values for all possible\n",
    "    actions given a state.\n",
    "\n",
    "    The input should have the shape of the state, and the output should have the same shape as\n",
    "    the action space since we want 1 Q value per possible action.\n",
    "\n",
    "    Args:\n",
    "\t\talpha: learning-rate\n",
    "\n",
    "\tReturns:\n",
    "\t\tq-net model\n",
    "    \"\"\"\n",
    "    x_data = np.linspace(0, 15, 16)\n",
    "    normalizer = keras.layers.Normalization(input_shape=[1, ], axis=None)\n",
    "    normalizer.adapt(np.array(x_data))\n",
    "\n",
    "    q_net = Sequential()\n",
    "    # We start with the normalizer, input shape is of size 1 (state)\n",
    "    q_net.add(normalizer)\n",
    "    # First hidden layer has 32 neurons\n",
    "    q_net.add(Dense(64, activation='relu', kernel_initializer='he_uniform'))\n",
    "    # The second hidden layer also have 32 neurons\n",
    "    q_net.add(Dense(64, activation='relu', kernel_initializer='he_uniform'))\n",
    "    # Since we have 4 possible actions, the output layer should be of size 4\n",
    "    q_net.add(Dense(4, activation='linear', kernel_initializer='he_uniform'))\n",
    "    q_net.compile(optimizer=Adam(learning_rate=alpha), loss='mse')\n",
    "    return q_net"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values for state 1: [[-1.4472475   2.6199214  -0.1604914  -0.41213113]]\n",
      "Q-value of state 1, action 1: 2.6199214458465576\n"
     ]
    }
   ],
   "source": [
    "# We can then create a dqn-model (it will be initialized with random weights)\n",
    "q_net_model = build_dqn_model()\n",
    "\n",
    "# And then we can \"predict\" the q-value outputs from a state s (in this case 1)\n",
    "state_input = tf.convert_to_tensor([1], dtype=tf.float32)\n",
    "pred = q_net_model.predict(state_input)\n",
    "print(f\"Q-values for state 1: {pred}\")\n",
    "\n",
    "# To get the q value for a specific action a (in this case action 1);\n",
    "q = pred[0][1]\n",
    "print(f\"Q-value of state 1, action 1: {q}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modifying q-learning functions\n",
    "\n",
    "We can reuse policy functions and playing functions from the q-learning agent (with a q-table backend), but we will need to modify them to take in the neural network instead:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "def dqn_optimal_policy(env: gym.Env, q_net: Sequential, s: int) -> int:\n",
    "    \"\"\"RL-policy for optimal play.\n",
    "\n",
    "    Args:\n",
    "        env: Frozen-lake Environment\n",
    "        q_net: q-network\n",
    "        s: state\n",
    "\n",
    "    Returns:\n",
    "        optimal action for given state and q-table.\n",
    "    \"\"\"\n",
    "    s_tensor = tf.convert_to_tensor([s], dtype=tf.float32)\n",
    "    q_values = q_net.predict(s_tensor)[0]\n",
    "    # print(f\"Q-values: {q_values}\")\n",
    "    return int(np.argmax(q_values))  # Return the argument (element number) with the highest q-value\n",
    "\n",
    "def dqn_epsilon_greedy_policy(env: gym.Env, q_net: Sequential, s: int, eps: float = 0.15) -> int:\n",
    "    \"\"\"RL-policy for exploration/exploitation play.\n",
    "\n",
    "    Args:\n",
    "        env: Frozen-lake Environment\n",
    "        q_net: q-network\n",
    "        s: state\n",
    "        eps: exploration chance\n",
    "\n",
    "    Returns:\n",
    "        either random action, or optimal action for given state and q-table.\n",
    "    \"\"\"\n",
    "    if np.random.rand() < eps:  # If a random number n is lower than eps:\n",
    "        return env.action_space.sample()  # Pick a random action\n",
    "    return dqn_optimal_policy(env, q_net, s)  # Otherwise, play optimally\n",
    "\n",
    "def dqn_decaying_epsilon_greedy_policy(env: gym.Env, q_net: Sequential, s: int, episode: int, max_episodes: int, max_eps: float = 0.95, min_eps: float = 0.01) -> int:\n",
    "    \"\"\"RL-policy for exploration/exploitation play.\n",
    "\n",
    "    Args:\n",
    "        env: Frozen-lake Environment\n",
    "        q_net: q-network\n",
    "        s: state\n",
    "        episode: current timestep\n",
    "        max_episodes: maximum timestep\n",
    "        max_eps: max exploration chance\n",
    "        min_eps: min exploration chance\n",
    "\n",
    "    Returns:\n",
    "        either random action, or optimal action for given state and q-table.\n",
    "    \"\"\"\n",
    "    max_episodes = int(max_episodes * 0.9)  # Testing with \"optimal play\" for last 10% of episodes\n",
    "    episode = min(episode, max_episodes)\n",
    "    eps = min_eps + (max_eps - min_eps) * ((max_episodes - episode) / max_episodes)\n",
    "    if np.random.rand() < eps:  # If a random number n is lower than eps:\n",
    "        return env.action_space.sample()  # Pick a random action\n",
    "    return dqn_optimal_policy(env, q_net, s)  # Otherwise, play optimally"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal action: 1\n"
     ]
    }
   ],
   "source": [
    "# We can test the optimal-policy:\n",
    "print(f\"Optimal action: {dqn_optimal_policy(environment, q_net_model, 1)}\") # Optimal action"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Playing with a DQN agent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "  (Down)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "  (Down)\n",
      "\u001B[41mS\u001B[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001B[41mF\u001B[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001B[41mH\u001B[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To play the game with a DQN-agent, we modify the \"Play FrozenLake with a q-table agent\", by replacing the policy with a DQN-policy:\n",
    "\n",
    "def dqn_play(max_steps: int = 20):\n",
    "    state, _ = environment.reset(return_info=True)  # Restart/initialize the environment\n",
    "    print(environment.render(mode=\"ansi\"))\n",
    "    for _ in range(max_steps):\n",
    "        action = dqn_optimal_policy(environment, q_net_model, state)  # Chose the optimal action based on values from the q-table\n",
    "        # print(f\"Action: {action}\")\n",
    "        new_state, reward, done, _ = environment.step(action)  # Play using that action\n",
    "        print(environment.render(mode=\"ansi\"))\n",
    "\n",
    "        # We stop the game if we are finished\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        state = new_state  # If not, replace the state with the new state before next step\n",
    "\n",
    "dqn_play()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experience Replay\n",
    "\n",
    "For training our network, we generally want to use batches sampled from a larger buffer of experiences\n",
    "\n",
    "### Replay buffer\n",
    "We can implement a buffer with the Experience class we implemented earlier:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from random import sample\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Replay buffer.\n",
    "\n",
    "    Stores and samples gameplay experiences\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_size: int = 2000) -> None:\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def store(self, experience: Experience) -> None:\n",
    "        \"\"\"Store a gameplay experience in the buffer.\n",
    "\n",
    "        Args:\n",
    "            experience: gameplay experience to store\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size: int = 32) -> list[Experience]:\n",
    "        \"\"\"Samples a list of gameplay experiences of (max) size batch_size.\n",
    "\n",
    "        Args:\n",
    "            batch_size: maximum size of the batch to sample\n",
    "\n",
    "        Returns:\n",
    "            Sampled batch of gameplay experiences\n",
    "        \"\"\"\n",
    "        batch_size = min(batch_size, len(self.buffer))\n",
    "        return sample(self.buffer, batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Storing experiences\n",
    "We can store experiences in the buffer simply by playing the game, as we did in the \"Playing with a DQN agent\" step:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "def collect_experiences(env: gym.Env, q_net: Sequential, buffer: ReplayBuffer, episode: int, max_episode: int, max_steps: int = 200) -> None:\n",
    "    \"\"\"Plays a single game/episode of the environment env, and stores all the transitions as experiences in the buffer.\n",
    "\n",
    "    Args:\n",
    "        env: OpenAI gym environment\n",
    "        q_net: Q-network\n",
    "        buffer: replay buffer\n",
    "        episode: current episode number (for decaying eps-greedy)\n",
    "        max_episode: max episode number (for decaying eps-greedy)\n",
    "        max_steps: max steps to play for in the environment\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    s, _ = environment.reset(return_info=True)  # Restart/initialize the environment\n",
    "    for _ in range(max_steps):\n",
    "        a = dqn_decaying_epsilon_greedy_policy(env, q_net, s, episode, max_episode)  # Chose the optimal action based on values from the q-table\n",
    "        s_new, r, d, _ = environment.step(a)  # Play using that action\n",
    "        if d and r == 0:\n",
    "            r = -1\n",
    "        experience = Experience(s, a, r, s_new, d)\n",
    "        buffer.store(experience)\n",
    "\n",
    "        # We stop the game if we are finished\n",
    "        if d:\n",
    "            break\n",
    "\n",
    "        s = s_new  # If not, replace the state with the new state before next step"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the q-net\n",
    "\n",
    "Now we need to be able to update the q-net, as we did with the q-table earlier in the notebook.\n",
    "(NB: This is not part of the pensum, but left for completeness)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluating the agent/q-net\n",
    "We should also be able to evaluate the q-net, so that we can say if it is doing well when training\n",
    "and to compare different models etc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "def evaluate_q_net(env: gym.Env, q_net: Sequential, episodes: int = 10, max_steps: int = 200) -> float:\n",
    "    \"\"\"Evaluates the performance of the given q-net.\n",
    "\n",
    "    Plays n games/episodes of the given environment and calculates the average reward.\n",
    "    Args:\n",
    "        env: the game environment\n",
    "        q_net: the q-net / agent\n",
    "        episodes: number of episodes to play\n",
    "        max_steps: max steps to play for in the environment\n",
    "\n",
    "    Returns:\n",
    "        average reward\n",
    "    \"\"\"\n",
    "    t_reward = 0.0\n",
    "    for _ in range(episodes):\n",
    "        s, _ = environment.reset(return_info=True)  # Restart/initialize the environment\n",
    "        ep_reward = 0.0\n",
    "        for _ in range(max_steps):\n",
    "            a = dqn_optimal_policy(env, q_net, s)  # Chose the optimal action\n",
    "            s_new, r, d, _ = environment.step(a)  # Play using that action\n",
    "            ep_reward += r\n",
    "            # We stop the game if we are finished\n",
    "            if d:\n",
    "                break\n",
    "\n",
    "            s = s_new  # If not, replace the state with the new state before next step\n",
    "        t_reward += ep_reward\n",
    "    return t_reward/episodes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q-Net Learning\n",
    "Finally, the replacement for the q-learning method:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "def dqn_utility(q_net: Sequential, s: int) -> int:\n",
    "    \"\"\"Utility function.\n",
    "\n",
    "    Args:\n",
    "        q_net: q-network\n",
    "        s: state\n",
    "\n",
    "    Returns:\n",
    "        q-value of optimal action for given state and q-net.\n",
    "    \"\"\"\n",
    "    s_tensor = tf.convert_to_tensor([s], dtype=tf.float32)\n",
    "    q_values = q_net.predict(s_tensor)[0]\n",
    "    return int(np.amax(q_values))  # Return the argument (element number) with the highest q-value\n",
    "\n",
    "def train(q_net: Sequential, batch: list[Experience], gamma: float = 0.98) -> float:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        q_net: q-net\n",
    "        batch: the batch to train on\n",
    "        gamma: discount-value\n",
    "\n",
    "    Returns:\n",
    "        trained q-net\n",
    "    \"\"\"\n",
    "    # We first create a list of all current q-values in the batch:\n",
    "    batch_states = [experience.state for experience in batch]\n",
    "    s_tensor = tf.convert_to_tensor(batch_states, dtype=tf.float32)\n",
    "    q_values = q_net.predict(s_tensor)\n",
    "\n",
    "    # We want to calculate the error over the q-values, so we make a copy to use as a target\n",
    "    target_q = np.copy(q_values)\n",
    "\n",
    "    # We then repeat for all utilities of the next states in the batch:\n",
    "    batch_ns = [experience.new_state for experience in batch]\n",
    "    ns_tensor = tf.convert_to_tensor(batch_ns, dtype=tf.float32)\n",
    "    utilities = q_net.predict(ns_tensor)\n",
    "    utilities = [np.amax(utility) for utility in utilities]\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        experience = batch[i]\n",
    "        target = experience.reward\n",
    "        if not experience.done:\n",
    "            # Error is similar to q-learning\n",
    "            target = experience.reward + gamma * utilities[i]\n",
    "\n",
    "        # What we would have predicted\n",
    "\n",
    "        # We update the prediction (to use as the error)\n",
    "        target_q[i][experience.action] = target\n",
    "    # Now we update the network, the fit function will take care of the rest of the update algorithm (learning-rate, error and gradient)\n",
    "    target_q = tf.convert_to_tensor(target_q, dtype=tf.float32)\n",
    "    training_history = q_net.fit(x=s_tensor, y=target_q, verbose=0)\n",
    "    loss = training_history.history['loss']\n",
    "    return loss\n",
    "\n",
    "def dqn_learning(env: gym.Env, q_net: Sequential, buffer: ReplayBuffer, min_buffer: int = 100, n_episodes: int = 10000, max_steps: int = 200) -> Sequential:\n",
    "    \"\"\"dqn implementation to update a q-net.\n",
    "\n",
    "\tArgs:\n",
    "\t\tenv: gym environment\n",
    "\t\tq_net: agent/q-net\n",
    "\t\tbuffer: The replay-buffer we will use\n",
    "\t\tmin_buffer: minimum buffer size before we start training\n",
    "\t\tn_episodes: number of episodes to train on\n",
    "\t\tmax_steps: maximum episode length\n",
    "\n",
    "\tReturns:\n",
    "\t\tupdated q-table\n",
    "    \"\"\"\n",
    "    # We first start by playing a few episodes so that we have some samples in our buffer\n",
    "    for episode in range(n_episodes):\n",
    "        collect_experiences(env, q_net, buffer, episode, n_episodes, max_steps=max_steps)  # Plays one episode and adds to buffer\n",
    "\n",
    "        if episode >= min_buffer:  # We only start updating the q-net after we have enough experiences to sample from\n",
    "            experience_batch = buffer.sample(256)\n",
    "            loss = train(q_net, experience_batch)\n",
    "            performance = evaluate_q_net(env, q_net)\n",
    "            print(f\"Episode: {episode}/{n_episodes}, the performance of the q-net is: {performance}, the loss is: {loss[0]}\")\n",
    "    return q_net"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100/5000, the performance of the q-net is: 0.0, the loss is: 1.3142683506011963\n",
      "Episode: 101/5000, the performance of the q-net is: 0.0, the loss is: 0.6105836033821106\n",
      "Episode: 102/5000, the performance of the q-net is: 0.0, the loss is: 0.27251121401786804\n",
      "Episode: 103/5000, the performance of the q-net is: 0.0, the loss is: 0.19604827463626862\n",
      "Episode: 104/5000, the performance of the q-net is: 0.0, the loss is: 0.14146272838115692\n",
      "Episode: 105/5000, the performance of the q-net is: 0.0, the loss is: 0.12943997979164124\n",
      "Episode: 106/5000, the performance of the q-net is: 0.0, the loss is: 0.10835181176662445\n",
      "Episode: 107/5000, the performance of the q-net is: 0.0, the loss is: 0.11646077781915665\n",
      "Episode: 108/5000, the performance of the q-net is: 0.0, the loss is: 0.10373982787132263\n",
      "Episode: 109/5000, the performance of the q-net is: 0.0, the loss is: 0.10603200644254684\n",
      "Episode: 110/5000, the performance of the q-net is: 0.0, the loss is: 0.11984729766845703\n",
      "Episode: 111/5000, the performance of the q-net is: 0.0, the loss is: 0.0902726873755455\n",
      "Episode: 112/5000, the performance of the q-net is: 0.0, the loss is: 0.09520713239908218\n",
      "Episode: 113/5000, the performance of the q-net is: 0.0, the loss is: 0.09038043767213821\n",
      "Episode: 114/5000, the performance of the q-net is: 0.0, the loss is: 0.07305225729942322\n",
      "Episode: 115/5000, the performance of the q-net is: 0.0, the loss is: 0.08122283220291138\n",
      "Episode: 116/5000, the performance of the q-net is: 0.0, the loss is: 0.08724427223205566\n",
      "Episode: 117/5000, the performance of the q-net is: 0.0, the loss is: 0.07379838824272156\n",
      "Episode: 118/5000, the performance of the q-net is: 0.0, the loss is: 0.05747376009821892\n",
      "Episode: 119/5000, the performance of the q-net is: 0.0, the loss is: 0.05803447216749191\n",
      "Episode: 120/5000, the performance of the q-net is: 0.0, the loss is: 0.06451678276062012\n",
      "Episode: 121/5000, the performance of the q-net is: 0.0, the loss is: 0.08035919070243835\n",
      "Episode: 122/5000, the performance of the q-net is: 0.0, the loss is: 0.0724673867225647\n",
      "Episode: 123/5000, the performance of the q-net is: 0.0, the loss is: 0.07122199237346649\n",
      "Episode: 124/5000, the performance of the q-net is: 0.0, the loss is: 0.06216362491250038\n",
      "Episode: 125/5000, the performance of the q-net is: 0.0, the loss is: 0.050858885049819946\n",
      "Episode: 126/5000, the performance of the q-net is: 0.0, the loss is: 0.052679143846035004\n",
      "Episode: 127/5000, the performance of the q-net is: 0.0, the loss is: 0.05571968853473663\n",
      "Episode: 128/5000, the performance of the q-net is: 0.0, the loss is: 0.06278210133314133\n",
      "Episode: 129/5000, the performance of the q-net is: 0.0, the loss is: 0.051477909088134766\n",
      "Episode: 130/5000, the performance of the q-net is: 0.0, the loss is: 0.051882460713386536\n",
      "Episode: 131/5000, the performance of the q-net is: 0.0, the loss is: 0.05020173266530037\n",
      "Episode: 132/5000, the performance of the q-net is: 0.0, the loss is: 0.04655427113175392\n",
      "Episode: 133/5000, the performance of the q-net is: 0.0, the loss is: 0.043451230973005295\n",
      "Episode: 134/5000, the performance of the q-net is: 0.0, the loss is: 0.04207426309585571\n",
      "Episode: 135/5000, the performance of the q-net is: 0.0, the loss is: 0.04991507902741432\n",
      "Episode: 136/5000, the performance of the q-net is: 0.0, the loss is: 0.04234936833381653\n",
      "Episode: 137/5000, the performance of the q-net is: 0.0, the loss is: 0.03645124286413193\n",
      "Episode: 138/5000, the performance of the q-net is: 0.0, the loss is: 0.03865516185760498\n",
      "Episode: 139/5000, the performance of the q-net is: 0.0, the loss is: 0.042180150747299194\n",
      "Episode: 140/5000, the performance of the q-net is: 0.0, the loss is: 0.043425463140010834\n",
      "Episode: 141/5000, the performance of the q-net is: 0.0, the loss is: 0.03160659596323967\n",
      "Episode: 142/5000, the performance of the q-net is: 0.1, the loss is: 0.033905621618032455\n",
      "Episode: 143/5000, the performance of the q-net is: 0.0, the loss is: 0.03391077369451523\n",
      "Episode: 144/5000, the performance of the q-net is: 0.0, the loss is: 0.041174937039613724\n",
      "Episode: 145/5000, the performance of the q-net is: 0.0, the loss is: 0.033848751336336136\n",
      "Episode: 146/5000, the performance of the q-net is: 0.0, the loss is: 0.03525936231017113\n",
      "Episode: 147/5000, the performance of the q-net is: 0.0, the loss is: 0.034014541655778885\n",
      "Episode: 148/5000, the performance of the q-net is: 0.0, the loss is: 0.03249400109052658\n",
      "Episode: 149/5000, the performance of the q-net is: 0.0, the loss is: 0.04196450486779213\n",
      "Episode: 150/5000, the performance of the q-net is: 0.0, the loss is: 0.028460239991545677\n",
      "Episode: 4951/5000, the performance of the q-net is: 0.4, the loss is: 0.00621964642778039\n",
      "Episode: 4952/5000, the performance of the q-net is: 0.7, the loss is: 0.006192932836711407\n",
      "Episode: 4953/5000, the performance of the q-net is: 0.7, the loss is: 0.00714219082146883\n",
      "Episode: 4954/5000, the performance of the q-net is: 0.0, the loss is: 0.005634555593132973\n",
      "Episode: 4955/5000, the performance of the q-net is: 0.7, the loss is: 0.0035239614080637693\n",
      "Episode: 4956/5000, the performance of the q-net is: 0.7, the loss is: 0.004233743064105511\n",
      "Episode: 4957/5000, the performance of the q-net is: 0.5, the loss is: 0.004027568269520998\n",
      "Episode: 4958/5000, the performance of the q-net is: 0.4, the loss is: 0.0037454203702509403\n",
      "Episode: 4959/5000, the performance of the q-net is: 0.5, the loss is: 0.003135203616693616\n",
      "Episode: 4960/5000, the performance of the q-net is: 0.4, the loss is: 0.0054098996333777905\n",
      "Episode: 4961/5000, the performance of the q-net is: 0.6, the loss is: 0.003230358473956585\n",
      "Episode: 4962/5000, the performance of the q-net is: 0.7, the loss is: 0.004135430790483952\n",
      "Episode: 4963/5000, the performance of the q-net is: 0.5, the loss is: 0.00562860444188118\n",
      "Episode: 4964/5000, the performance of the q-net is: 0.5, the loss is: 0.005542464088648558\n",
      "Episode: 4965/5000, the performance of the q-net is: 0.8, the loss is: 0.004585573449730873\n",
      "Episode: 4966/5000, the performance of the q-net is: 0.6, the loss is: 0.004574805963784456\n",
      "Episode: 4967/5000, the performance of the q-net is: 0.5, the loss is: 0.006259877234697342\n",
      "Episode: 4968/5000, the performance of the q-net is: 0.5, the loss is: 0.006347776856273413\n",
      "Episode: 4969/5000, the performance of the q-net is: 0.8, the loss is: 0.007379941176623106\n",
      "Episode: 4970/5000, the performance of the q-net is: 0.7, the loss is: 0.006015035789459944\n",
      "Episode: 4971/5000, the performance of the q-net is: 0.4, the loss is: 0.00487101124599576\n",
      "Episode: 4972/5000, the performance of the q-net is: 0.7, the loss is: 0.005395535379648209\n",
      "Episode: 4973/5000, the performance of the q-net is: 0.6, the loss is: 0.007004934828728437\n",
      "Episode: 4974/5000, the performance of the q-net is: 0.6, the loss is: 0.005197949707508087\n",
      "Episode: 4975/5000, the performance of the q-net is: 0.6, the loss is: 0.005243800580501556\n",
      "Episode: 4976/5000, the performance of the q-net is: 0.8, the loss is: 0.007885539904236794\n",
      "Episode: 4977/5000, the performance of the q-net is: 0.4, the loss is: 0.0069995662197470665\n",
      "Episode: 4978/5000, the performance of the q-net is: 0.6, the loss is: 0.007332462351769209\n",
      "Episode: 4979/5000, the performance of the q-net is: 0.9, the loss is: 0.005202441476285458\n",
      "Episode: 4980/5000, the performance of the q-net is: 0.8, the loss is: 0.004764316137880087\n",
      "Episode: 4981/5000, the performance of the q-net is: 0.5, the loss is: 0.004577931948006153\n",
      "Episode: 4982/5000, the performance of the q-net is: 0.3, the loss is: 0.007711200043559074\n",
      "Episode: 4983/5000, the performance of the q-net is: 0.6, the loss is: 0.003854326205328107\n",
      "Episode: 4984/5000, the performance of the q-net is: 0.8, the loss is: 0.004870261065661907\n",
      "Episode: 4985/5000, the performance of the q-net is: 0.4, the loss is: 0.006163762882351875\n",
      "Episode: 4986/5000, the performance of the q-net is: 0.5, the loss is: 0.004012854304164648\n",
      "Episode: 4987/5000, the performance of the q-net is: 0.8, the loss is: 0.004816528409719467\n",
      "Episode: 4988/5000, the performance of the q-net is: 0.7, the loss is: 0.004488919861614704\n",
      "Episode: 4989/5000, the performance of the q-net is: 0.3, the loss is: 0.0030766797717660666\n",
      "Episode: 4990/5000, the performance of the q-net is: 0.1, the loss is: 0.0034004009794443846\n",
      "Episode: 4991/5000, the performance of the q-net is: 0.6, the loss is: 0.0025007545482367277\n",
      "Episode: 4992/5000, the performance of the q-net is: 0.3, the loss is: 0.005312258377671242\n",
      "Episode: 4993/5000, the performance of the q-net is: 0.5, the loss is: 0.0028251754119992256\n",
      "Episode: 4994/5000, the performance of the q-net is: 0.8, the loss is: 0.006363464519381523\n",
      "Episode: 4995/5000, the performance of the q-net is: 0.4, the loss is: 0.00480245053768158\n",
      "Episode: 4996/5000, the performance of the q-net is: 0.7, the loss is: 0.006599657237529755\n",
      "Episode: 4997/5000, the performance of the q-net is: 0.6, the loss is: 0.00468854047358036\n",
      "Episode: 4998/5000, the performance of the q-net is: 0.4, the loss is: 0.007462379988282919\n",
      "Episode: 4999/5000, the performance of the q-net is: 0.7, the loss is: 0.004785140044987202\n"
     ]
    }
   ],
   "source": [
    "# Now to train:\n",
    "replay_buffer = ReplayBuffer(max_size=512)\n",
    "q_net_model = dqn_learning(environment, q_net_model, replay_buffer, n_episodes=5000)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now compare the q-table and the q-net:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State 0: \n",
      "    q-table: [0.54 0.39 0.24 0.29] \n",
      "    q-net: [[ 0.19 -0.07 -0.17  0.01]]\n",
      "State 1: \n",
      "    q-table: [0.   0.06 0.06 0.36] \n",
      "    q-net: [[ 0.1  -0.11 -0.19  0.01]]\n",
      "State 2: \n",
      "    q-table: [0.18 0.15 0.04 0.15] \n",
      "    q-net: [[-0.04 -0.18 -0.23 -0.01]]\n",
      "State 3: \n",
      "    q-table: [0.   0.01 0.01 0.15] \n",
      "    q-net: [[-0.15 -0.2  -0.24 -0.07]]\n",
      "State 4: \n",
      "    q-table: [0.64 0.01 0.01 0.38] \n",
      "    q-net: [[ 0.23 -0.17 -0.11 -0.4 ]]\n",
      "State 5: \n",
      "    q-table: [0. 0. 0. 0.] \n",
      "    q-net: [[ 0.06 -0.3  -0.15 -0.55]]\n",
      "State 6: \n",
      "    q-table: [0.13 0.   0.   0.  ] \n",
      "    q-net: [[-0.15 -0.61 -0.15 -0.84]]\n",
      "State 7: \n",
      "    q-table: [0. 0. 0. 0.] \n",
      "    q-net: [[-0.43 -1.24 -0.27  0.03]]\n",
      "State 8: \n",
      "    q-table: [0.12 0.1  0.02 0.59] \n",
      "    q-net: [[ 0.07 -0.17 -0.05  0.31]]\n",
      "State 9: \n",
      "    q-table: [0.11 0.73 0.11 0.09] \n",
      "    q-net: [[ 0.1   0.36  0.08 -0.07]]\n",
      "State 10: \n",
      "    q-table: [0.94 0.   0.   0.06] \n",
      "    q-net: [[ 0.32  0.18  0.19 -0.12]]\n",
      "State 11: \n",
      "    q-table: [0. 0. 0. 0.] \n",
      "    q-net: [[ 0.32  0.09  0.28 -0.11]]\n",
      "State 12: \n",
      "    q-table: [0. 0. 0. 0.] \n",
      "    q-net: [[ 0.33  0.02  0.37 -0.09]]\n",
      "State 13: \n",
      "    q-table: [0.63 0.7  0.83 0.11] \n",
      "    q-net: [[ 0.38  0.02  0.48 -0.09]]\n",
      "State 14: \n",
      "    q-table: [0.29 1.   0.2  0.35] \n",
      "    q-net: [[ 0.45  0.06  0.61 -0.08]]\n",
      "State 15: \n",
      "    q-table: [0. 0. 0. 0.] \n",
      "    q-net: [[ 0.51  0.13  0.75 -0.05]]\n"
     ]
    }
   ],
   "source": [
    "def compare_q(q_net: Sequential, q_sa: np.array):\n",
    "    for s in range(16):\n",
    "        s_tensor = tf.convert_to_tensor([s], dtype=tf.float32)\n",
    "        q_values = q_net.predict(s_tensor)\n",
    "        print(f\"State {s}: \\n    q-table: {np.round(q_sa[s],2)} \\n    q-net: {np.round(q_values, 2)}\")\n",
    "compare_q(q_net_model, q_table)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
