{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Frozen-Lake with a DQN (Deep Q-Network) agent\n",
    "\n",
    "## Neural Network predictions\n",
    "\n",
    "While we in the previous step used a q-table as backend, the problem can also be solved by function approximation with a neural network.\n",
    "\n",
    "### Building the model\n",
    "\n",
    "We can start by building the network model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.optimizer_v2.adam import Adam\n",
    "\n",
    "def build_dqn_model(alpha: float = 0.001) -> Sequential:\n",
    "    \"\"\"Builds a deep neural net which predicts the Q values for all possible\n",
    "    actions given a state.\n",
    "\n",
    "    The input should have the shape of the state, and the output should have the same shape as\n",
    "    the action space since we want 1 Q value per possible action.\n",
    "\n",
    "    Args:\n",
    "\t\talpha: learning-rate\n",
    "\n",
    "\tReturns:\n",
    "\t\tq-net model\n",
    "    \"\"\"\n",
    "    x_data = np.linspace(0, 15, 16)\n",
    "    normalizer = keras.layers.Normalization(input_shape=[1, ], axis=None)\n",
    "    normalizer.adapt(np.array(x_data))\n",
    "\n",
    "    q_net = Sequential()\n",
    "    # We start with the normalizer, input shape is of size 1 (state)\n",
    "    q_net.add(normalizer)\n",
    "    # First hidden layer has 32 neurons\n",
    "    q_net.add(Dense(64, activation='relu', kernel_initializer='he_uniform'))\n",
    "    # The second hidden layer also have 32 neurons\n",
    "    q_net.add(Dense(64, activation='relu', kernel_initializer='he_uniform'))\n",
    "    # Since we have 4 possible actions, the output layer should be of size 4\n",
    "    q_net.add(Dense(4, activation='linear', kernel_initializer='he_uniform'))\n",
    "    q_net.compile(optimizer=Adam(learning_rate=alpha), loss='mse')\n",
    "    return q_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# We can then create a dqn-model (it will be initialized with random weights)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m q_net_model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_dqn_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# And then we can \"predict\" the q-value outputs from a state s (in this case 1)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m state_input \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor([\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "Cell \u001b[0;32mIn [1], line 21\u001b[0m, in \u001b[0;36mbuild_dqn_model\u001b[0;34m(alpha)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_dqn_model\u001b[39m(alpha: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequential:\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Builds a deep neural net which predicts the Q values for all possible\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    actions given a state.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m\t\tq-net model\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     x_data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m     22\u001b[0m     normalizer \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mNormalization(input_shape\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, ], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m     normalizer\u001b[38;5;241m.\u001b[39madapt(np\u001b[38;5;241m.\u001b[39marray(x_data))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# We can then create a dqn-model (it will be initialized with random weights)\n",
    "q_net_model = build_dqn_model()\n",
    "\n",
    "# And then we can \"predict\" the q-value outputs from a state s (in this case 1)\n",
    "state_input = tf.convert_to_tensor([1], dtype=tf.float32)\n",
    "pred = q_net_model.predict(state_input)\n",
    "print(f\"Q-values for state 1: {pred}\")\n",
    "\n",
    "# To get the q value for a specific action a (in this case action 1);\n",
    "q = pred[0][1]\n",
    "print(f\"Q-value of state 1, action 1: {q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Modifying q-learning functions\n",
    "\n",
    "We can reuse policy functions and playing functions from the q-learning agent (with a q-table backend), but we will need to modify them to take in the neural network instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def dqn_optimal_policy(env: gym.Env, q_net: Sequential, s: int) -> int:\n",
    "    \"\"\"RL-policy for optimal play.\n",
    "\n",
    "    Args:\n",
    "        env: Frozen-lake Environment\n",
    "        q_net: q-network\n",
    "        s: state\n",
    "\n",
    "    Returns:\n",
    "        optimal action for given state and q-table.\n",
    "    \"\"\"\n",
    "    s_tensor = tf.convert_to_tensor([s], dtype=tf.float32)\n",
    "    q_values = q_net.predict(s_tensor)[0]\n",
    "    # print(f\"Q-values: {q_values}\")\n",
    "    return int(np.argmax(q_values))  # Return the argument (element number) with the highest q-value\n",
    "\n",
    "def dqn_epsilon_greedy_policy(env: gym.Env, q_net: Sequential, s: int, eps: float = 0.15) -> int:\n",
    "    \"\"\"RL-policy for exploration/exploitation play.\n",
    "\n",
    "    Args:\n",
    "        env: Frozen-lake Environment\n",
    "        q_net: q-network\n",
    "        s: state\n",
    "        eps: exploration chance\n",
    "\n",
    "    Returns:\n",
    "        either random action, or optimal action for given state and q-table.\n",
    "    \"\"\"\n",
    "    if np.random.rand() < eps:  # If a random number n is lower than eps:\n",
    "        return env.action_space.sample()  # Pick a random action\n",
    "    return dqn_optimal_policy(env, q_net, s)  # Otherwise, play optimally\n",
    "\n",
    "def dqn_decaying_epsilon_greedy_policy(env: gym.Env, q_net: Sequential, s: int, episode: int, max_episodes: int, max_eps: float = 0.95, min_eps: float = 0.01) -> int:\n",
    "    \"\"\"RL-policy for exploration/exploitation play.\n",
    "\n",
    "    Args:\n",
    "        env: Frozen-lake Environment\n",
    "        q_net: q-network\n",
    "        s: state\n",
    "        episode: current timestep\n",
    "        max_episodes: maximum timestep\n",
    "        max_eps: max exploration chance\n",
    "        min_eps: min exploration chance\n",
    "\n",
    "    Returns:\n",
    "        either random action, or optimal action for given state and q-table.\n",
    "    \"\"\"\n",
    "    max_episodes = int(max_episodes * 0.9)  # Testing with \"optimal play\" for last 10% of episodes\n",
    "    episode = min(episode, max_episodes)\n",
    "    eps = min_eps + (max_eps - min_eps) * ((max_episodes - episode) / max_episodes)\n",
    "    if np.random.rand() < eps:  # If a random number n is lower than eps:\n",
    "        return env.action_space.sample()  # Pick a random action\n",
    "    return dqn_optimal_policy(env, q_net, s)  # Otherwise, play optimally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We can test the optimal-policy:\n",
    "print(f\"Optimal action: {dqn_optimal_policy(environment, q_net_model, 1)}\") # Optimal action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Playing with a DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# To play the game with a DQN-agent, we modify the \"Play FrozenLake with a q-table agent\", by replacing the policy with a DQN-policy:\n",
    "\n",
    "def dqn_play(max_steps: int = 20):\n",
    "    state, _ = environment.reset(return_info=True)  # Restart/initialize the environment\n",
    "    print(environment.render(mode=\"ansi\"))\n",
    "    for _ in range(max_steps):\n",
    "        action = dqn_optimal_policy(environment, q_net_model, state)  # Chose the optimal action based on values from the q-table\n",
    "        # print(f\"Action: {action}\")\n",
    "        new_state, reward, done, _ = environment.step(action)  # Play using that action\n",
    "        print(environment.render(mode=\"ansi\"))\n",
    "\n",
    "        # We stop the game if we are finished\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        state = new_state  # If not, replace the state with the new state before next step\n",
    "\n",
    "dqn_play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Experience Replay\n",
    "\n",
    "For training our network, we generally want to use batches sampled from a larger buffer of experiences\n",
    "\n",
    "### Replay buffer\n",
    "We can implement a buffer with the Experience class we implemented earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from random import sample\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Replay buffer.\n",
    "\n",
    "    Stores and samples gameplay experiences\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_size: int = 2000) -> None:\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def store(self, experience: Experience) -> None:\n",
    "        \"\"\"Store a gameplay experience in the buffer.\n",
    "\n",
    "        Args:\n",
    "            experience: gameplay experience to store\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size: int = 32) -> list[Experience]:\n",
    "        \"\"\"Samples a list of gameplay experiences of (max) size batch_size.\n",
    "\n",
    "        Args:\n",
    "            batch_size: maximum size of the batch to sample\n",
    "\n",
    "        Returns:\n",
    "            Sampled batch of gameplay experiences\n",
    "        \"\"\"\n",
    "        batch_size = min(batch_size, len(self.buffer))\n",
    "        return sample(self.buffer, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Storing experiences\n",
    "We can store experiences in the buffer simply by playing the game, as we did in the \"Playing with a DQN agent\" step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def collect_experiences(env: gym.Env, q_net: Sequential, buffer: ReplayBuffer, episode: int, max_episode: int, max_steps: int = 200) -> None:\n",
    "    \"\"\"Plays a single game/episode of the environment env, and stores all the transitions as experiences in the buffer.\n",
    "\n",
    "    Args:\n",
    "        env: OpenAI gym environment\n",
    "        q_net: Q-network\n",
    "        buffer: replay buffer\n",
    "        episode: current episode number (for decaying eps-greedy)\n",
    "        max_episode: max episode number (for decaying eps-greedy)\n",
    "        max_steps: max steps to play for in the environment\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    s, _ = environment.reset(return_info=True)  # Restart/initialize the environment\n",
    "    for _ in range(max_steps):\n",
    "        a = dqn_decaying_epsilon_greedy_policy(env, q_net, s, episode, max_episode)  # Chose the optimal action based on values from the q-table\n",
    "        s_new, r, d, _ = environment.step(a)  # Play using that action\n",
    "        if d and r == 0:\n",
    "            r = -1\n",
    "        experience = Experience(s, a, r, s_new, d)\n",
    "        buffer.store(experience)\n",
    "\n",
    "        # We stop the game if we are finished\n",
    "        if d:\n",
    "            break\n",
    "\n",
    "        s = s_new  # If not, replace the state with the new state before next step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training the q-net\n",
    "\n",
    "Now we need to be able to update the q-net, as we did with the q-table earlier in the notebook.\n",
    "(NB: This is not part of the pensum, but left for completeness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Evaluating the agent/q-net\n",
    "We should also be able to evaluate the q-net, so that we can say if it is doing well when training\n",
    "and to compare different models etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_q_net(env: gym.Env, q_net: Sequential, episodes: int = 10, max_steps: int = 200) -> float:\n",
    "    \"\"\"Evaluates the performance of the given q-net.\n",
    "\n",
    "    Plays n games/episodes of the given environment and calculates the average reward.\n",
    "    Args:\n",
    "        env: the game environment\n",
    "        q_net: the q-net / agent\n",
    "        episodes: number of episodes to play\n",
    "        max_steps: max steps to play for in the environment\n",
    "\n",
    "    Returns:\n",
    "        average reward\n",
    "    \"\"\"\n",
    "    t_reward = 0.0\n",
    "    for _ in range(episodes):\n",
    "        s, _ = environment.reset(return_info=True)  # Restart/initialize the environment\n",
    "        ep_reward = 0.0\n",
    "        for _ in range(max_steps):\n",
    "            a = dqn_optimal_policy(env, q_net, s)  # Chose the optimal action\n",
    "            s_new, r, d, _ = environment.step(a)  # Play using that action\n",
    "            ep_reward += r\n",
    "            # We stop the game if we are finished\n",
    "            if d:\n",
    "                break\n",
    "\n",
    "            s = s_new  # If not, replace the state with the new state before next step\n",
    "        t_reward += ep_reward\n",
    "    return t_reward/episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Q-Net Learning\n",
    "Finally, the replacement for the q-learning method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def dqn_utility(q_net: Sequential, s: int) -> int:\n",
    "    \"\"\"Utility function.\n",
    "\n",
    "    Args:\n",
    "        q_net: q-network\n",
    "        s: state\n",
    "\n",
    "    Returns:\n",
    "        q-value of optimal action for given state and q-net.\n",
    "    \"\"\"\n",
    "    s_tensor = tf.convert_to_tensor([s], dtype=tf.float32)\n",
    "    q_values = q_net.predict(s_tensor)[0]\n",
    "    return int(np.amax(q_values))  # Return the argument (element number) with the highest q-value\n",
    "\n",
    "def train(q_net: Sequential, batch: list[Experience], gamma: float = 0.98) -> float:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        q_net: q-net\n",
    "        batch: the batch to train on\n",
    "        gamma: discount-value\n",
    "\n",
    "    Returns:\n",
    "        trained q-net\n",
    "    \"\"\"\n",
    "    # We first create a list of all current q-values in the batch:\n",
    "    batch_states = [experience.state for experience in batch]\n",
    "    s_tensor = tf.convert_to_tensor(batch_states, dtype=tf.float32)\n",
    "    q_values = q_net.predict(s_tensor)\n",
    "\n",
    "    # We want to calculate the error over the q-values, so we make a copy to use as a target\n",
    "    target_q = np.copy(q_values)\n",
    "\n",
    "    # We then repeat for all utilities of the next states in the batch:\n",
    "    batch_ns = [experience.new_state for experience in batch]\n",
    "    ns_tensor = tf.convert_to_tensor(batch_ns, dtype=tf.float32)\n",
    "    utilities = q_net.predict(ns_tensor)\n",
    "    utilities = [np.amax(utility) for utility in utilities]\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        experience = batch[i]\n",
    "        target = experience.reward\n",
    "        if not experience.done:\n",
    "            # Error is similar to q-learning\n",
    "            target = experience.reward + gamma * utilities[i]\n",
    "\n",
    "        # What we would have predicted\n",
    "\n",
    "        # We update the prediction (to use as the error)\n",
    "        target_q[i][experience.action] = target\n",
    "    # Now we update the network, the fit function will take care of the rest of the update algorithm (learning-rate, error and gradient)\n",
    "    target_q = tf.convert_to_tensor(target_q, dtype=tf.float32)\n",
    "    training_history = q_net.fit(x=s_tensor, y=target_q, verbose=0)\n",
    "    loss = training_history.history['loss']\n",
    "    return loss\n",
    "\n",
    "def dqn_learning(env: gym.Env, q_net: Sequential, buffer: ReplayBuffer, min_buffer: int = 100, n_episodes: int = 10000, max_steps: int = 200) -> Sequential:\n",
    "    \"\"\"dqn implementation to update a q-net.\n",
    "\n",
    "\tArgs:\n",
    "\t\tenv: gym environment\n",
    "\t\tq_net: agent/q-net\n",
    "\t\tbuffer: The replay-buffer we will use\n",
    "\t\tmin_buffer: minimum buffer size before we start training\n",
    "\t\tn_episodes: number of episodes to train on\n",
    "\t\tmax_steps: maximum episode length\n",
    "\n",
    "\tReturns:\n",
    "\t\tupdated q-table\n",
    "    \"\"\"\n",
    "    # We first start by playing a few episodes so that we have some samples in our buffer\n",
    "    for episode in range(n_episodes):\n",
    "        collect_experiences(env, q_net, buffer, episode, n_episodes, max_steps=max_steps)  # Plays one episode and adds to buffer\n",
    "\n",
    "        if episode >= min_buffer:  # We only start updating the q-net after we have enough experiences to sample from\n",
    "            experience_batch = buffer.sample(256)\n",
    "            loss = train(q_net, experience_batch)\n",
    "            performance = evaluate_q_net(env, q_net)\n",
    "            print(f\"Episode: {episode}/{n_episodes}, the performance of the q-net is: {performance}, the loss is: {loss[0]}\")\n",
    "    return q_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Now to train:\n",
    "replay_buffer = ReplayBuffer(max_size=512)\n",
    "q_net_model = dqn_learning(environment, q_net_model, replay_buffer, n_episodes=5000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can now compare the q-table and the q-net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compare_q(q_net: Sequential, q_sa: np.array):\n",
    "    for s in range(16):\n",
    "        s_tensor = tf.convert_to_tensor([s], dtype=tf.float32)\n",
    "        q_values = q_net.predict(s_tensor)\n",
    "        print(f\"State {s}: \\n    q-table: {np.round(q_sa[s],2)} \\n    q-net: {np.round(q_values, 2)}\")\n",
    "compare_q(q_net_model, q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
