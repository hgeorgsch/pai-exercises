{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Frozen-Lake with a DQN (Deep Q-Network) agent\n",
    "\n",
    "## Neural Network predictions\n",
    "\n",
    "While we in the previous step used a q-table as backend, the problem can also be solved by function approximation with a neural network.\n",
    "\n",
    "### Building the model\n",
    "\n",
    "We can start by building the network model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.optimizer_v2.adam import Adam\n",
    "\n",
    "def build_dqn_model(alpha: float = 0.001) -> Sequential:\n",
    "    \"\"\"Builds a deep neural net which predicts the Q values for all possible\n",
    "    actions given a state.\n",
    "\n",
    "    The input should have the shape of the state, and the output should have the same shape as\n",
    "    the action space since we want 1 Q value per possible action.\n",
    "\n",
    "    Args:\n",
    "\t\talpha: learning-rate\n",
    "\n",
    "\tReturns:\n",
    "\t\tq-net model\n",
    "    \"\"\"\n",
    "    x_data = np.linspace(0, 15, 16)\n",
    "    normalizer = keras.layers.Normalization(input_shape=[1, ], axis=None)\n",
    "    normalizer.adapt(np.array(x_data))\n",
    "\n",
    "    q_net = Sequential()\n",
    "    # We start with the normalizer, input shape is of size 1 (state)\n",
    "    q_net.add(normalizer)\n",
    "    # First hidden layer has 32 neurons\n",
    "    q_net.add(Dense(64, activation='relu', kernel_initializer='he_uniform'))\n",
    "    # The second hidden layer also have 32 neurons\n",
    "    q_net.add(Dense(64, activation='relu', kernel_initializer='he_uniform'))\n",
    "    # Since we have 4 possible actions, the output layer should be of size 4\n",
    "    q_net.add(Dense(4, activation='linear', kernel_initializer='he_uniform'))\n",
    "    q_net.compile(optimizer=Adam(learning_rate=alpha), loss='mse')\n",
    "    return q_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values for state 1: [[-1.4472475   2.6199214  -0.1604914  -0.41213113]]\n",
      "Q-value of state 1, action 1: 2.6199214458465576\n"
     ]
    }
   ],
   "source": [
    "# We can then create a dqn-model (it will be initialized with random weights)\n",
    "q_net_model = build_dqn_model()\n",
    "\n",
    "# And then we can \"predict\" the q-value outputs from a state s (in this case 1)\n",
    "state_input = tf.convert_to_tensor([1], dtype=tf.float32)\n",
    "pred = q_net_model.predict(state_input)\n",
    "print(f\"Q-values for state 1: {pred}\")\n",
    "\n",
    "# To get the q value for a specific action a (in this case action 1);\n",
    "q = pred[0][1]\n",
    "print(f\"Q-value of state 1, action 1: {q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Modifying q-learning functions\n",
    "\n",
    "We can reuse policy functions and playing functions from the q-learning agent (with a q-table backend), but we will need to modify them to take in the neural network instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def dqn_optimal_policy(env: gym.Env, q_net: Sequential, s: int) -> int:\n",
    "    \"\"\"RL-policy for optimal play.\n",
    "\n",
    "    Args:\n",
    "        env: Frozen-lake Environment\n",
    "        q_net: q-network\n",
    "        s: state\n",
    "\n",
    "    Returns:\n",
    "        optimal action for given state and q-table.\n",
    "    \"\"\"\n",
    "    s_tensor = tf.convert_to_tensor([s], dtype=tf.float32)\n",
    "    q_values = q_net.predict(s_tensor)[0]\n",
    "    # print(f\"Q-values: {q_values}\")\n",
    "    return int(np.argmax(q_values))  # Return the argument (element number) with the highest q-value\n",
    "\n",
    "def dqn_epsilon_greedy_policy(env: gym.Env, q_net: Sequential, s: int, eps: float = 0.15) -> int:\n",
    "    \"\"\"RL-policy for exploration/exploitation play.\n",
    "\n",
    "    Args:\n",
    "        env: Frozen-lake Environment\n",
    "        q_net: q-network\n",
    "        s: state\n",
    "        eps: exploration chance\n",
    "\n",
    "    Returns:\n",
    "        either random action, or optimal action for given state and q-table.\n",
    "    \"\"\"\n",
    "    if np.random.rand() < eps:  # If a random number n is lower than eps:\n",
    "        return env.action_space.sample()  # Pick a random action\n",
    "    return dqn_optimal_policy(env, q_net, s)  # Otherwise, play optimally\n",
    "\n",
    "def dqn_decaying_epsilon_greedy_policy(env: gym.Env, q_net: Sequential, s: int, episode: int, max_episodes: int, max_eps: float = 0.95, min_eps: float = 0.01) -> int:\n",
    "    \"\"\"RL-policy for exploration/exploitation play.\n",
    "\n",
    "    Args:\n",
    "        env: Frozen-lake Environment\n",
    "        q_net: q-network\n",
    "        s: state\n",
    "        episode: current timestep\n",
    "        max_episodes: maximum timestep\n",
    "        max_eps: max exploration chance\n",
    "        min_eps: min exploration chance\n",
    "\n",
    "    Returns:\n",
    "        either random action, or optimal action for given state and q-table.\n",
    "    \"\"\"\n",
    "    max_episodes = int(max_episodes * 0.9)  # Testing with \"optimal play\" for last 10% of episodes\n",
    "    episode = min(episode, max_episodes)\n",
    "    eps = min_eps + (max_eps - min_eps) * ((max_episodes - episode) / max_episodes)\n",
    "    if np.random.rand() < eps:  # If a random number n is lower than eps:\n",
    "        return env.action_space.sample()  # Pick a random action\n",
    "    return dqn_optimal_policy(env, q_net, s)  # Otherwise, play optimally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal action: 1\n"
     ]
    }
   ],
   "source": [
    "# We can test the optimal-policy:\n",
    "print(f\"Optimal action: {dqn_optimal_policy(environment, q_net_model, 1)}\") # Optimal action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Playing with a DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "  (Down)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "  (Down)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To play the game with a DQN-agent, we modify the \"Play FrozenLake with a q-table agent\", by replacing the policy with a DQN-policy:\n",
    "\n",
    "def dqn_play(max_steps: int = 20):\n",
    "    state, _ = environment.reset(return_info=True)  # Restart/initialize the environment\n",
    "    print(environment.render(mode=\"ansi\"))\n",
    "    for _ in range(max_steps):\n",
    "        action = dqn_optimal_policy(environment, q_net_model, state)  # Chose the optimal action based on values from the q-table\n",
    "        # print(f\"Action: {action}\")\n",
    "        new_state, reward, done, _ = environment.step(action)  # Play using that action\n",
    "        print(environment.render(mode=\"ansi\"))\n",
    "\n",
    "        # We stop the game if we are finished\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        state = new_state  # If not, replace the state with the new state before next step\n",
    "\n",
    "dqn_play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Experience Replay\n",
    "\n",
    "For training our network, we generally want to use batches sampled from a larger buffer of experiences\n",
    "\n",
    "### Replay buffer\n",
    "We can implement a buffer with the Experience class we implemented earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from random import sample\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Replay buffer.\n",
    "\n",
    "    Stores and samples gameplay experiences\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_size: int = 2000) -> None:\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def store(self, experience: Experience) -> None:\n",
    "        \"\"\"Store a gameplay experience in the buffer.\n",
    "\n",
    "        Args:\n",
    "            experience: gameplay experience to store\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size: int = 32) -> list[Experience]:\n",
    "        \"\"\"Samples a list of gameplay experiences of (max) size batch_size.\n",
    "\n",
    "        Args:\n",
    "            batch_size: maximum size of the batch to sample\n",
    "\n",
    "        Returns:\n",
    "            Sampled batch of gameplay experiences\n",
    "        \"\"\"\n",
    "        batch_size = min(batch_size, len(self.buffer))\n",
    "        return sample(self.buffer, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Storing experiences\n",
    "We can store experiences in the buffer simply by playing the game, as we did in the \"Playing with a DQN agent\" step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def collect_experiences(env: gym.Env, q_net: Sequential, buffer: ReplayBuffer, episode: int, max_episode: int, max_steps: int = 200) -> None:\n",
    "    \"\"\"Plays a single game/episode of the environment env, and stores all the transitions as experiences in the buffer.\n",
    "\n",
    "    Args:\n",
    "        env: OpenAI gym environment\n",
    "        q_net: Q-network\n",
    "        buffer: replay buffer\n",
    "        episode: current episode number (for decaying eps-greedy)\n",
    "        max_episode: max episode number (for decaying eps-greedy)\n",
    "        max_steps: max steps to play for in the environment\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    s, _ = environment.reset(return_info=True)  # Restart/initialize the environment\n",
    "    for _ in range(max_steps):\n",
    "        a = dqn_decaying_epsilon_greedy_policy(env, q_net, s, episode, max_episode)  # Chose the optimal action based on values from the q-table\n",
    "        s_new, r, d, _ = environment.step(a)  # Play using that action\n",
    "        if d and r == 0:\n",
    "            r = -1\n",
    "        experience = Experience(s, a, r, s_new, d)\n",
    "        buffer.store(experience)\n",
    "\n",
    "        # We stop the game if we are finished\n",
    "        if d:\n",
    "            break\n",
    "\n",
    "        s = s_new  # If not, replace the state with the new state before next step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training the q-net\n",
    "\n",
    "Now we need to be able to update the q-net, as we did with the q-table earlier in the notebook.\n",
    "(NB: This is not part of the pensum, but left for completeness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Evaluating the agent/q-net\n",
    "We should also be able to evaluate the q-net, so that we can say if it is doing well when training\n",
    "and to compare different models etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_q_net(env: gym.Env, q_net: Sequential, episodes: int = 10, max_steps: int = 200) -> float:\n",
    "    \"\"\"Evaluates the performance of the given q-net.\n",
    "\n",
    "    Plays n games/episodes of the given environment and calculates the average reward.\n",
    "    Args:\n",
    "        env: the game environment\n",
    "        q_net: the q-net / agent\n",
    "        episodes: number of episodes to play\n",
    "        max_steps: max steps to play for in the environment\n",
    "\n",
    "    Returns:\n",
    "        average reward\n",
    "    \"\"\"\n",
    "    t_reward = 0.0\n",
    "    for _ in range(episodes):\n",
    "        s, _ = environment.reset(return_info=True)  # Restart/initialize the environment\n",
    "        ep_reward = 0.0\n",
    "        for _ in range(max_steps):\n",
    "            a = dqn_optimal_policy(env, q_net, s)  # Chose the optimal action\n",
    "            s_new, r, d, _ = environment.step(a)  # Play using that action\n",
    "            ep_reward += r\n",
    "            # We stop the game if we are finished\n",
    "            if d:\n",
    "                break\n",
    "\n",
    "            s = s_new  # If not, replace the state with the new state before next step\n",
    "        t_reward += ep_reward\n",
    "    return t_reward/episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Q-Net Learning\n",
    "Finally, the replacement for the q-learning method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def dqn_utility(q_net: Sequential, s: int) -> int:\n",
    "    \"\"\"Utility function.\n",
    "\n",
    "    Args:\n",
    "        q_net: q-network\n",
    "        s: state\n",
    "\n",
    "    Returns:\n",
    "        q-value of optimal action for given state and q-net.\n",
    "    \"\"\"\n",
    "    s_tensor = tf.convert_to_tensor([s], dtype=tf.float32)\n",
    "    q_values = q_net.predict(s_tensor)[0]\n",
    "    return int(np.amax(q_values))  # Return the argument (element number) with the highest q-value\n",
    "\n",
    "def train(q_net: Sequential, batch: list[Experience], gamma: float = 0.98) -> float:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        q_net: q-net\n",
    "        batch: the batch to train on\n",
    "        gamma: discount-value\n",
    "\n",
    "    Returns:\n",
    "        trained q-net\n",
    "    \"\"\"\n",
    "    # We first create a list of all current q-values in the batch:\n",
    "    batch_states = [experience.state for experience in batch]\n",
    "    s_tensor = tf.convert_to_tensor(batch_states, dtype=tf.float32)\n",
    "    q_values = q_net.predict(s_tensor)\n",
    "\n",
    "    # We want to calculate the error over the q-values, so we make a copy to use as a target\n",
    "    target_q = np.copy(q_values)\n",
    "\n",
    "    # We then repeat for all utilities of the next states in the batch:\n",
    "    batch_ns = [experience.new_state for experience in batch]\n",
    "    ns_tensor = tf.convert_to_tensor(batch_ns, dtype=tf.float32)\n",
    "    utilities = q_net.predict(ns_tensor)\n",
    "    utilities = [np.amax(utility) for utility in utilities]\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        experience = batch[i]\n",
    "        target = experience.reward\n",
    "        if not experience.done:\n",
    "            # Error is similar to q-learning\n",
    "            target = experience.reward + gamma * utilities[i]\n",
    "\n",
    "        # What we would have predicted\n",
    "\n",
    "        # We update the prediction (to use as the error)\n",
    "        target_q[i][experience.action] = target\n",
    "    # Now we update the network, the fit function will take care of the rest of the update algorithm (learning-rate, error and gradient)\n",
    "    target_q = tf.convert_to_tensor(target_q, dtype=tf.float32)\n",
    "    training_history = q_net.fit(x=s_tensor, y=target_q, verbose=0)\n",
    "    loss = training_history.history['loss']\n",
    "    return loss\n",
    "\n",
    "def dqn_learning(env: gym.Env, q_net: Sequential, buffer: ReplayBuffer, min_buffer: int = 100, n_episodes: int = 10000, max_steps: int = 200) -> Sequential:\n",
    "    \"\"\"dqn implementation to update a q-net.\n",
    "\n",
    "\tArgs:\n",
    "\t\tenv: gym environment\n",
    "\t\tq_net: agent/q-net\n",
    "\t\tbuffer: The replay-buffer we will use\n",
    "\t\tmin_buffer: minimum buffer size before we start training\n",
    "\t\tn_episodes: number of episodes to train on\n",
    "\t\tmax_steps: maximum episode length\n",
    "\n",
    "\tReturns:\n",
    "\t\tupdated q-table\n",
    "    \"\"\"\n",
    "    # We first start by playing a few episodes so that we have some samples in our buffer\n",
    "    for episode in range(n_episodes):\n",
    "        collect_experiences(env, q_net, buffer, episode, n_episodes, max_steps=max_steps)  # Plays one episode and adds to buffer\n",
    "\n",
    "        if episode >= min_buffer:  # We only start updating the q-net after we have enough experiences to sample from\n",
    "            experience_batch = buffer.sample(256)\n",
    "            loss = train(q_net, experience_batch)\n",
    "            performance = evaluate_q_net(env, q_net)\n",
    "            print(f\"Episode: {episode}/{n_episodes}, the performance of the q-net is: {performance}, the loss is: {loss[0]}\")\n",
    "    return q_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100/5000, the performance of the q-net is: 0.0, the loss is: 1.3142683506011963\n",
      "Episode: 101/5000, the performance of the q-net is: 0.0, the loss is: 0.6105836033821106\n",
      "Episode: 102/5000, the performance of the q-net is: 0.0, the loss is: 0.27251121401786804\n",
      "Episode: 103/5000, the performance of the q-net is: 0.0, the loss is: 0.19604827463626862\n",
      "Episode: 104/5000, the performance of the q-net is: 0.0, the loss is: 0.14146272838115692\n",
      "Episode: 105/5000, the performance of the q-net is: 0.0, the loss is: 0.12943997979164124\n",
      "Episode: 106/5000, the performance of the q-net is: 0.0, the loss is: 0.10835181176662445\n",
      "Episode: 107/5000, the performance of the q-net is: 0.0, the loss is: 0.11646077781915665\n",
      "Episode: 108/5000, the performance of the q-net is: 0.0, the loss is: 0.10373982787132263\n",
      "Episode: 109/5000, the performance of the q-net is: 0.0, the loss is: 0.10603200644254684\n",
      "Episode: 110/5000, the performance of the q-net is: 0.0, the loss is: 0.11984729766845703\n",
      "Episode: 111/5000, the performance of the q-net is: 0.0, the loss is: 0.0902726873755455\n",
      "Episode: 112/5000, the performance of the q-net is: 0.0, the loss is: 0.09520713239908218\n",
      "Episode: 113/5000, the performance of the q-net is: 0.0, the loss is: 0.09038043767213821\n",
      "Episode: 114/5000, the performance of the q-net is: 0.0, the loss is: 0.07305225729942322\n",
      "Episode: 115/5000, the performance of the q-net is: 0.0, the loss is: 0.08122283220291138\n",
      "Episode: 116/5000, the performance of the q-net is: 0.0, the loss is: 0.08724427223205566\n",
      "Episode: 117/5000, the performance of the q-net is: 0.0, the loss is: 0.07379838824272156\n",
      "Episode: 118/5000, the performance of the q-net is: 0.0, the loss is: 0.05747376009821892\n",
      "Episode: 119/5000, the performance of the q-net is: 0.0, the loss is: 0.05803447216749191\n",
      "Episode: 120/5000, the performance of the q-net is: 0.0, the loss is: 0.06451678276062012\n",
      "Episode: 121/5000, the performance of the q-net is: 0.0, the loss is: 0.08035919070243835\n",
      "Episode: 122/5000, the performance of the q-net is: 0.0, the loss is: 0.0724673867225647\n",
      "Episode: 123/5000, the performance of the q-net is: 0.0, the loss is: 0.07122199237346649\n",
      "Episode: 124/5000, the performance of the q-net is: 0.0, the loss is: 0.06216362491250038\n",
      "Episode: 125/5000, the performance of the q-net is: 0.0, the loss is: 0.050858885049819946\n",
      "Episode: 126/5000, the performance of the q-net is: 0.0, the loss is: 0.052679143846035004\n",
      "Episode: 127/5000, the performance of the q-net is: 0.0, the loss is: 0.05571968853473663\n",
      "Episode: 128/5000, the performance of the q-net is: 0.0, the loss is: 0.06278210133314133\n",
      "Episode: 129/5000, the performance of the q-net is: 0.0, the loss is: 0.051477909088134766\n",
      "Episode: 130/5000, the performance of the q-net is: 0.0, the loss is: 0.051882460713386536\n",
      "Episode: 131/5000, the performance of the q-net is: 0.0, the loss is: 0.05020173266530037\n",
      "Episode: 132/5000, the performance of the q-net is: 0.0, the loss is: 0.04655427113175392\n",
      "Episode: 133/5000, the performance of the q-net is: 0.0, the loss is: 0.043451230973005295\n",
      "Episode: 134/5000, the performance of the q-net is: 0.0, the loss is: 0.04207426309585571\n",
      "Episode: 135/5000, the performance of the q-net is: 0.0, the loss is: 0.04991507902741432\n",
      "Episode: 136/5000, the performance of the q-net is: 0.0, the loss is: 0.04234936833381653\n",
      "Episode: 137/5000, the performance of the q-net is: 0.0, the loss is: 0.03645124286413193\n",
      "Episode: 138/5000, the performance of the q-net is: 0.0, the loss is: 0.03865516185760498\n",
      "Episode: 139/5000, the performance of the q-net is: 0.0, the loss is: 0.042180150747299194\n",
      "Episode: 140/5000, the performance of the q-net is: 0.0, the loss is: 0.043425463140010834\n",
      "Episode: 141/5000, the performance of the q-net is: 0.0, the loss is: 0.03160659596323967\n",
      "Episode: 142/5000, the performance of the q-net is: 0.1, the loss is: 0.033905621618032455\n",
      "Episode: 143/5000, the performance of the q-net is: 0.0, the loss is: 0.03391077369451523\n",
      "Episode: 144/5000, the performance of the q-net is: 0.0, the loss is: 0.041174937039613724\n",
      "Episode: 145/5000, the performance of the q-net is: 0.0, the loss is: 0.033848751336336136\n",
      "Episode: 146/5000, the performance of the q-net is: 0.0, the loss is: 0.03525936231017113\n",
      "Episode: 147/5000, the performance of the q-net is: 0.0, the loss is: 0.034014541655778885\n",
      "Episode: 148/5000, the performance of the q-net is: 0.0, the loss is: 0.03249400109052658\n",
      "Episode: 149/5000, the performance of the q-net is: 0.0, the loss is: 0.04196450486779213\n",
      "Episode: 150/5000, the performance of the q-net is: 0.0, the loss is: 0.028460239991545677\n",
      "Episode: 4951/5000, the performance of the q-net is: 0.4, the loss is: 0.00621964642778039\n",
      "Episode: 4952/5000, the performance of the q-net is: 0.7, the loss is: 0.006192932836711407\n",
      "Episode: 4953/5000, the performance of the q-net is: 0.7, the loss is: 0.00714219082146883\n",
      "Episode: 4954/5000, the performance of the q-net is: 0.0, the loss is: 0.005634555593132973\n",
      "Episode: 4955/5000, the performance of the q-net is: 0.7, the loss is: 0.0035239614080637693\n",
      "Episode: 4956/5000, the performance of the q-net is: 0.7, the loss is: 0.004233743064105511\n",
      "Episode: 4957/5000, the performance of the q-net is: 0.5, the loss is: 0.004027568269520998\n",
      "Episode: 4958/5000, the performance of the q-net is: 0.4, the loss is: 0.0037454203702509403\n",
      "Episode: 4959/5000, the performance of the q-net is: 0.5, the loss is: 0.003135203616693616\n",
      "Episode: 4960/5000, the performance of the q-net is: 0.4, the loss is: 0.0054098996333777905\n",
      "Episode: 4961/5000, the performance of the q-net is: 0.6, the loss is: 0.003230358473956585\n",
      "Episode: 4962/5000, the performance of the q-net is: 0.7, the loss is: 0.004135430790483952\n",
      "Episode: 4963/5000, the performance of the q-net is: 0.5, the loss is: 0.00562860444188118\n",
      "Episode: 4964/5000, the performance of the q-net is: 0.5, the loss is: 0.005542464088648558\n",
      "Episode: 4965/5000, the performance of the q-net is: 0.8, the loss is: 0.004585573449730873\n",
      "Episode: 4966/5000, the performance of the q-net is: 0.6, the loss is: 0.004574805963784456\n",
      "Episode: 4967/5000, the performance of the q-net is: 0.5, the loss is: 0.006259877234697342\n",
      "Episode: 4968/5000, the performance of the q-net is: 0.5, the loss is: 0.006347776856273413\n",
      "Episode: 4969/5000, the performance of the q-net is: 0.8, the loss is: 0.007379941176623106\n",
      "Episode: 4970/5000, the performance of the q-net is: 0.7, the loss is: 0.006015035789459944\n",
      "Episode: 4971/5000, the performance of the q-net is: 0.4, the loss is: 0.00487101124599576\n",
      "Episode: 4972/5000, the performance of the q-net is: 0.7, the loss is: 0.005395535379648209\n",
      "Episode: 4973/5000, the performance of the q-net is: 0.6, the loss is: 0.007004934828728437\n",
      "Episode: 4974/5000, the performance of the q-net is: 0.6, the loss is: 0.005197949707508087\n",
      "Episode: 4975/5000, the performance of the q-net is: 0.6, the loss is: 0.005243800580501556\n",
      "Episode: 4976/5000, the performance of the q-net is: 0.8, the loss is: 0.007885539904236794\n",
      "Episode: 4977/5000, the performance of the q-net is: 0.4, the loss is: 0.0069995662197470665\n",
      "Episode: 4978/5000, the performance of the q-net is: 0.6, the loss is: 0.007332462351769209\n",
      "Episode: 4979/5000, the performance of the q-net is: 0.9, the loss is: 0.005202441476285458\n",
      "Episode: 4980/5000, the performance of the q-net is: 0.8, the loss is: 0.004764316137880087\n",
      "Episode: 4981/5000, the performance of the q-net is: 0.5, the loss is: 0.004577931948006153\n",
      "Episode: 4982/5000, the performance of the q-net is: 0.3, the loss is: 0.007711200043559074\n",
      "Episode: 4983/5000, the performance of the q-net is: 0.6, the loss is: 0.003854326205328107\n",
      "Episode: 4984/5000, the performance of the q-net is: 0.8, the loss is: 0.004870261065661907\n",
      "Episode: 4985/5000, the performance of the q-net is: 0.4, the loss is: 0.006163762882351875\n",
      "Episode: 4986/5000, the performance of the q-net is: 0.5, the loss is: 0.004012854304164648\n",
      "Episode: 4987/5000, the performance of the q-net is: 0.8, the loss is: 0.004816528409719467\n",
      "Episode: 4988/5000, the performance of the q-net is: 0.7, the loss is: 0.004488919861614704\n",
      "Episode: 4989/5000, the performance of the q-net is: 0.3, the loss is: 0.0030766797717660666\n",
      "Episode: 4990/5000, the performance of the q-net is: 0.1, the loss is: 0.0034004009794443846\n",
      "Episode: 4991/5000, the performance of the q-net is: 0.6, the loss is: 0.0025007545482367277\n",
      "Episode: 4992/5000, the performance of the q-net is: 0.3, the loss is: 0.005312258377671242\n",
      "Episode: 4993/5000, the performance of the q-net is: 0.5, the loss is: 0.0028251754119992256\n",
      "Episode: 4994/5000, the performance of the q-net is: 0.8, the loss is: 0.006363464519381523\n",
      "Episode: 4995/5000, the performance of the q-net is: 0.4, the loss is: 0.00480245053768158\n",
      "Episode: 4996/5000, the performance of the q-net is: 0.7, the loss is: 0.006599657237529755\n",
      "Episode: 4997/5000, the performance of the q-net is: 0.6, the loss is: 0.00468854047358036\n",
      "Episode: 4998/5000, the performance of the q-net is: 0.4, the loss is: 0.007462379988282919\n",
      "Episode: 4999/5000, the performance of the q-net is: 0.7, the loss is: 0.004785140044987202\n"
     ]
    }
   ],
   "source": [
    "# Now to train:\n",
    "replay_buffer = ReplayBuffer(max_size=512)\n",
    "q_net_model = dqn_learning(environment, q_net_model, replay_buffer, n_episodes=5000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can now compare the q-table and the q-net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State 0: \n",
      "    q-table: [0.54 0.39 0.24 0.29] \n",
      "    q-net: [[ 0.19 -0.07 -0.17  0.01]]\n",
      "State 1: \n",
      "    q-table: [0.   0.06 0.06 0.36] \n",
      "    q-net: [[ 0.1  -0.11 -0.19  0.01]]\n",
      "State 2: \n",
      "    q-table: [0.18 0.15 0.04 0.15] \n",
      "    q-net: [[-0.04 -0.18 -0.23 -0.01]]\n",
      "State 3: \n",
      "    q-table: [0.   0.01 0.01 0.15] \n",
      "    q-net: [[-0.15 -0.2  -0.24 -0.07]]\n",
      "State 4: \n",
      "    q-table: [0.64 0.01 0.01 0.38] \n",
      "    q-net: [[ 0.23 -0.17 -0.11 -0.4 ]]\n",
      "State 5: \n",
      "    q-table: [0. 0. 0. 0.] \n",
      "    q-net: [[ 0.06 -0.3  -0.15 -0.55]]\n",
      "State 6: \n",
      "    q-table: [0.13 0.   0.   0.  ] \n",
      "    q-net: [[-0.15 -0.61 -0.15 -0.84]]\n",
      "State 7: \n",
      "    q-table: [0. 0. 0. 0.] \n",
      "    q-net: [[-0.43 -1.24 -0.27  0.03]]\n",
      "State 8: \n",
      "    q-table: [0.12 0.1  0.02 0.59] \n",
      "    q-net: [[ 0.07 -0.17 -0.05  0.31]]\n",
      "State 9: \n",
      "    q-table: [0.11 0.73 0.11 0.09] \n",
      "    q-net: [[ 0.1   0.36  0.08 -0.07]]\n",
      "State 10: \n",
      "    q-table: [0.94 0.   0.   0.06] \n",
      "    q-net: [[ 0.32  0.18  0.19 -0.12]]\n",
      "State 11: \n",
      "    q-table: [0. 0. 0. 0.] \n",
      "    q-net: [[ 0.32  0.09  0.28 -0.11]]\n",
      "State 12: \n",
      "    q-table: [0. 0. 0. 0.] \n",
      "    q-net: [[ 0.33  0.02  0.37 -0.09]]\n",
      "State 13: \n",
      "    q-table: [0.63 0.7  0.83 0.11] \n",
      "    q-net: [[ 0.38  0.02  0.48 -0.09]]\n",
      "State 14: \n",
      "    q-table: [0.29 1.   0.2  0.35] \n",
      "    q-net: [[ 0.45  0.06  0.61 -0.08]]\n",
      "State 15: \n",
      "    q-table: [0. 0. 0. 0.] \n",
      "    q-net: [[ 0.51  0.13  0.75 -0.05]]\n"
     ]
    }
   ],
   "source": [
    "def compare_q(q_net: Sequential, q_sa: np.array):\n",
    "    for s in range(16):\n",
    "        s_tensor = tf.convert_to_tensor([s], dtype=tf.float32)\n",
    "        q_values = q_net.predict(s_tensor)\n",
    "        print(f\"State {s}: \\n    q-table: {np.round(q_sa[s],2)} \\n    q-net: {np.round(q_values, 2)}\")\n",
    "compare_q(q_net_model, q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
